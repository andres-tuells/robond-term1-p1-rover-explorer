{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Search and Sample Return\n",
    "\n",
    "The goal of this project is to build a program that allows a rover inside a simulator to navigate the terrain and pickup samples.\n",
    "\n",
    "To fullfil this project the program has 2 steps. A perception step and a decision step.\n",
    "In the perception step the program analyses the front camera view with the rover position  to extract features from the enviroment.This features are navigable terrain, obstacles and samples.\n",
    "In the decision step the program uses the information available about obstacles, terrain, samples and rover's state to take an action. The final goal is to collect all samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perception step\n",
    "\n",
    "![Perception transform](./assets/perception_transform.png)\n",
    "\n",
    "The first image is the image from the camera. The second one is the previos image with a perspective transform to have a point of view from above. In the third one we use color thresh to detect the color pixels from the terrain. We do three types of extraction: navigable terrain, obstacle detection and sample detection. We use different color thresh parameters to differentiate between this 3 features. \n",
    "In the four image we change the cartesian coordinates from the image to polar coordinates with the camera as the center. In this way we have the same perspective as the rover and we can calculate distance and angle of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worldmap\n",
    "\n",
    "We do further image processing to map the enviroment features with the worldmap. We have the image with a point of view from above. We have the position and angle of the rover. We have the polar coordinates of the features from the pov of the rover. We transform the polar coordinates of the rover to cartessian coordinates in the worldmap. In this way we can map the features to the worldmap.\n",
    "We collect all the camera images to a video:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"./output/test_mapping.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format('./output/test_mapping.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first image is the camera view. The second one is the from above view. The third one is the worldmap view + features. In blue is the explored terrain. Rocks are marked as white."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision step\n",
    "\n",
    "In decision step we use the image information + the rover state to decide with action to takes. Possible actions are move forward, steer, stop and pickup samples.\n",
    "\n",
    "The biggest problem is to avoid is to stuck the rover. I use 2 techniques here. One is if the rover doesn't move in a long time it turns almost 180 degrees to continue the exploration.\n",
    "The other technique is no use the image and detect the mean angle of navegable terrain and steer the wheels in that direction. It also uses the distance of the navigable terrain to brake if necessary.\n",
    "In the same way, it there is a sample detected it calculates the mean angle and distance to get closer. When it's near it stops and pickups the sample.\n",
    "I also added a random steer from time to time to wide the exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
